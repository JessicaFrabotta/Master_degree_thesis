# Design and Implementation of a Novel Grasping System for a Humanoid Robot in QiBullet Simulator


## ğŸ“– Project Overview
This thesis presents the design and implementation of a **grasping system** for the **Softbank Robotics Pepper humanoid robot** within the **QiBullet simulator**.  

The system enables Pepper to **detect, localize, and grasp unknown objects** without relying on external markers or additional sensors, using only the hardware it is already equipped with.  

This work is significant because grasping is not a typical skill for social humanoid robots like Pepper. Enhancing this capability not only poses a technical challenge but also broadens the potential applications of the robot.

---

## âš™ï¸ Technical Framework
The grasping system is fully **modular** and developed in **Python**.  

### ğŸ”‘ Key Modules
- **Object Detection** â†’ YOLOv10 (small version) is used to identify and outline target objects in images from Pepperâ€™s cameras.  
- **Depth Estimation** â†’ Monocular approach with **Depth Anything V2**, generating a depth map from a single image.  
- **Coordinate Mapping** â†’ Transforms 2D image coordinates into the robotâ€™s **3D base coordinate system**.  
- **Path Planning** â†’ Optimizes the robotâ€™s arm trajectory (position, orientation, and distance) to achieve the best grasping pose.  
- **Inverse Kinematics** â†’ Implemented using the **Playful Kinematics** framework. Computes joint configurations for Pepperâ€™s **arms and lower body** to ensure efficient and natural movements.

---

## ğŸ¤– Robot and Simulator
- **Robot**: Softbank Robotics **Pepper**  
  - 20 Degrees of Freedom (DOF)  
  - Two 640Ã—480 cameras (forehead and mouth)  
  - Limited hands (fully open/close, max 200g payload)  
  - VisionLab model includes stereo cameras behind the eyes (not supported in QiBullet)  

- **Simulator**: **QiBullet** (based on Bullet physics engine)  
  - Simulates gravity, friction, and physical interactions  
  - Calibrated simulated cameras to match real Pepper  

---

## ğŸ”„ System Workflow
1. Pepper rotates to detect a target object (a small bottle on a table).  
2. **Forehead camera** captures images at 15 FPS â†’ sent to a Flask server for detection and depth estimation.  
3. Once closer, the **mouth camera** provides detailed close-range images â†’ processed again for improved accuracy.  
4. **Path Planning** adjusts robotâ€™s position and orientation.  
5. **Inverse Kinematics** executes the grasp.  

---

## ğŸ“Š Results
- **25 simulations** were conducted.  
- A grasp was considered successful if Pepper could reach and lift the bottle.  
- **Success rate: 72%** ğŸ‰  

While the simulator lacks real-world variability (lighting, noise), the system showed strong **robustness and reliability**, making it suitable for future real-world testing on the physical Pepper robot.  

The **modular design** also allows adaptation to other humanoid platforms.

---

## ğŸš€ Future Work
- Test system on the **real Pepper** robot at VisionLab.  
- Improve grasping for **more complex objects**.  
- Extend modular framework to **other humanoid robots**.  

---
